<!DOCTYPE html>
<html>
<head>
  <title></title>
 <meta name="viewport" content="width=device-width,height=device-height">
  <script src="https://d3js.org/d3.v4.min.js" charset="utf-8"></script>
    <link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,700,900" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet">
<style>
::-webkit-scrollbar {
    display:none;
    width: 0px;
    background: transparent; /* make scrollbar transparent */
}
::-webkit-scrollbar-track
{
    background-color: transparent;
}
::-webkit-scrollbar-thumb
{
    background-color: transparent;
}
body, html {
  font-family: 'Playfair Display', serif;
  width:100%;
  overflow-x:hidden;
  margin:0;
  left:0;
  top:0;
/*  scroll-snap-type: y mandatory;*/
}
h1 {
  font-weight:400;
  font-size:3em;
    color:#572EFF;
}
em {
  font-style:normal;
  font-weight:900;
}
a{
  color:#572EFF;
  text-decoration:none;
}
.section {
  width:100%;
  height:100vh;
/*  scroll-snap-align: start;*/
  animation: fadeIn 1s ease-in-out;
}
.text-section {
  opacity:0;
  width:90%;
  height:auto;
  padding:5%;
  line-height:2em;
  transform:translateY(5em);
  transition:0.5s ease-in-out;
  animation:fadeIn 1s ease-in-out;
}
.quote {
  font-size:1.5em;
  color:#9C9198;
  border-left:solid #572EFF 0.25em;
  padding-left:5%;

}
#title {
  height:100vh;
}

.scroll {
  font-stretch:expanded;
  position:absolute;
  width:100%;
  bottom:5%;
  text-align:center;
  font-size:6em;
  color:#572EFF;
  cursor:pointer;
  transition:0.5s all ease-in-out;
}
.scroll:hover {
  transform:scale(1.2);
  transform-origin:50% 30%;
}
table {
  width:90%;
}
th {
  text-align:left;
}
td {
}
@keyframes fadeIn {
  0% {opacity:0;}
  100% {opacity:1;}
}

/*iphone5*/
@media screen and (max-width:320px){
  .scroll {
    bottom:-5%;
  }
}
/*mobile*/
@media screen and (max-width:767px){
/*  body, html {
  scroll-snap-type: none;
  }
  .section {
    scroll-snap-align: none;
  }*/
  .scroll {
    bottom:0%;
  }
}
</style>
</head>
<body>

  <div class="section" id="title">
     <iframe src="title.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
     <div class="scroll">&#xfe40;</div>
  </div>

  <div class="text-section" id="start">
      <h1>Big Picture</h1>
      <h2>Automating Predictions</h2>
      <p>
      The Allegheny Family Screening Tool is a computer program that predicts whether a child will later have to be placed into foster care. It's been used in Allegheny County, Pennsylvania, since August 2016. When a child is referred to the county as at risk of abuse or neglect, the program analyzes administrative records and then outputs a score from 1 to 20, where a higher score represents a higher risk that the child will later have to be placed into foster care. Child welfare workers use the score to help them decide whether to investigate a case further.
      <br><br>
      Travel search engines like Kayak or Google Flights predict whether a flight will go up or down in price. Farecast, which launched in 2004 and was acquired by Microsoft a few years later, was the first to offer such a service. When you look up a flight, these search engines analyze price records and then predict whether the flight's price will go up or down over some time interval, perhaps along with a measure of confidence in the prediction. People use the predictions to help them decide when to buy a ticket.
      <br><br>
      </p>
  </div>

  <div class="text-section">
      <p>
      <em>Compas</em> is a computer program that predicts whether a person will go on to commit a crime. It's widely used in the justice system in the United States. When someone is arrested in a jurisdiction that uses the program, prison staff input information about them and then  <em>compas</em> outputs a score from 1 to 10, where a higher score represents a higher risk that they'll go on to commit a crime. In some states, judges use the scores to help them make decisions about bond amounts, sentencing and parole.
      <br><br>
      The Allegheny Family Screening Tool, flight price forecasts and <em>compas</em> are examples of <em>automated prediction tools.</em> Such tools are becoming more and more common in all sorts of fields: from healthcare to the criminal justice system, from education to sports analytics, from social services to shopping. They typically go hand in hand with big data, machine learning and AI.
      <br><br>
      On the one hand, you might hope that using automated prediction tools will lead to better predictions: more accurate, because based on huge datasets; more accountable, because arrived at by explicit calculation; more fair, because free of human biases. Just as AlphaGo plays better Go than any person, so too will automated prediction tools make better predictions than any person --- or so you might hope.
      <br><br>
      On the other hand, you might worry that using these tools will lead to worse predictions: less accurate, because using fixed and ill-chosen criteria; less accountable, because the details are proprietary or opaque; less fair, because reflecting the biases of the dataset and the designers. Just as people are better at writing jokes or doing cryptic crosswords than any program, so too with making predictions --- or so you might worry.
      </p>
      </div>

    <div class="text-section">
      <p>
      <h2>Aim of the post</h2>
      This post is about <em>fairness</em>. In particular, it’s about some interesting recent results (e.g. Chouldechova 2017, Kleinberg et al. 2017), results which came out of attempts to check whether particular automated prediction tools were fair, but which seem to have a more general consequence: that in a wide variety of situations it’s impossible to make fair predictions. As Kleinberg et al. put it in their abstract: “These results suggest some of the ways in which key notions of fairness are incompatible with each other”.
      <br><br>
      The aim of the post is to make these results accessible to a general audience, without sacrificing rigor or generality. By the end, you'll understand the results and be well-positioned to follow up on them yourself. And by smoothing away the technicalities, I hope to help bring out some of the conceptual issues.
      <br><br>
      The literature on fairness in automated prediction tools is extensive and rapidly growing. For example: some researchers develop formal definitions of fairness; some try to produce fairer prediction tools; and some argue that we should forget about formalizing fairness and instead focus on the broader social and political implications of using automated prediction tools in a given context. The results which I explain in this post belong to the first of these research programs: formal definitions of fairness.
      </p>
      </div>

    <div class="text-section">
      <p> 
      <h2>An example: COMPAS</h2>
      The best way into the topic is through a concrete example. Take one of the prediction tools above, <em>compas</em>, which predicts whether a person will go on to commit a crime.

      The information input to <em>compas</em> is gathered from a questionnaire, some of which is filled out by prison staff and some by the subject themselves. Here's <a href="documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPASCORE.html">a questionnaire used in Wisconsin.</a> Questions include: "How many times has this person been sentenced to jail for 30 days or more?", "If you lived with both parents and they later separated, how old were you at the time?", "How often do you have barely enough money to get by?", "To what extent do you agree or disagree that a hungry person has a right to steal?". After you input the information from the questionnaire, <em>compas</em> outputs a score. We don't know how exactly it does this because it's produced and sold by a private company (formerly Northpointe, now <a href="https://www.equivant.com/northpointe-suite">Equivant </a> and the details of how it works are proprietary.
      <br><br>
      How should we evaluate <em>compas</em>? Here's a straightforward approach: gather the risk scores <em>compas</em> assigned in lots of actual cases, find out in each case whether the subject did go on to commit a crime, and check the data for any worrying patterns. 
      <br><br>
      Of course, it's <a href="https://www.themarshallproject.org/2014/12/04/the-misleading-math-of-recidivism">not in fact straightforward</a> to check whether someone did go on to commit a crime. So most studies use a proxy, such as whether someone was re-arrested within, say, two years, or got another conviction. Using proxies like these raises all sorts of serious questions: How well do the proxies track the truth? How reliable is the data about the proxies themselves? How do we combine studies which used different proxies?  And there is a further, more general, worry too: making a prediction about someone might causally affect whether that very prediction comes true, because, for example, it might affect whether that person is incarcerated or not, or their parole conditions. Despite these caveats, the idea is clear: a direct way to evaluate <em>compas</em> is to analyze how its predictions held up in a large number of cases.
      <br><br>
      Investigative journalists at ProPublica did exactly that. They gathered data about over 7000 cases in Broward County, Florida, and checked it for any worrying patterns. They found some. In May 2016, they published <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">an article</a> in which they claimed that <em>compas</em> is ``biased against blacks''. Here are some of the patterns they found:
      <br><br>
      <table>
        <tr>
          <th>Criteria</th>
          <th>White</th> 
          <th>African American</th>
        </tr>
        <tr>
          <td>score was 5-10 but didn't go on to commit a crime</td>
          <td>23.5%</td> 
          <td>44.9%</td>
        </tr>
        <tr>
          <td>score was 1-4 but did go on to commit a crime</td>
          <td>47.7%</td> 
          <td>28.0%</td>
        </tr>
      </table>
      <br><br>
      What is the table telling us? Focus first on the white subjects in ProPublica's data. Around a quarter of them were judged to have a high risk of committing a crime (a score of 5-10) but in fact didn't commit a crime. Around half of them were judged to have a low risk of committing a crime (a score of 1-4) but in fact did. Now focus on the black subjects. The situation is roughly the opposite. Around half were judged to have a high risk of committing a crime but in fact didn't. Around a quarter were judged to have a low risk of committing a crime but in fact did. 
      <br><br>
      We might say, quite naturally, that when <em>compas</em> gives a high score to someone who doesn't commit a crime or a low score to someone who does, that it's made a mistake. When <em>compas</em> makes a mistake with white people, it tends to be by giving low scores. When it makes a mistake with black people, it tends to be by giving high scores. 
      <br><br>
      The statistics look damning. They seem to show, as ProPublica claimed, that <em>compas</em> is biased against black people. (Here is Northpointe’s response. And here is ProPublica’s response to that.)
      <br><br>
      However, it turns out that things are not so clear. No one disputes ProPublica's statistics. But people do dispute whether the statistics demonstrate that <em>compas</em> is biased. Why? Because some recent results show that no matter how scores are assigned --- whether by human or machine, whether using algorithm, experience or intuition, whether based on a little evidence or a lot --- similar statistics will hold. Statistics like these are unavoidable. If statistics like these show that a prediction tool is unfair, then fair prediction tools are impossible.
      <br><br>
</p>
  </div>

  <div class="text-section">
    <h1>Crates and Boxes</h1>
    <h2>Crate-and-boxes problems</h2>
    <p>
    To think through the results, we’ll use a clean and clear abstraction: <em>crate-and-boxes problems</em>. The abstraction, free from distracting complexities, will help us think, in general, about how predictions can generate statistics like ProPublica’s. A crate-and-boxes problem works like this:
    <br><br>
    <div class="quote">Mac is facing a crate full of locked boxes. Each box is labeled with a letter. Inside each box is a ball, which is either red or green, and a cube, which is either blue or yellow. Mac draws a box at random from the crate, reads its label, and predicts the color of the ball inside.</div>
    <br><br>
    </p>
  </div>

  <div class="section" id="setup">
    <iframe src="1.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

  <div class="text-section">
    <p>
    To specify a particular crate-and-boxes problem, we need to specify which boxes are in the crate. For example, we need to specify how many boxes are labeled <em>A</em> and contain a red ball and a blue cube, how many boxes are labeled <em>B</em> and contain a green ball and a yellow cube, and so on. We can specify all these things concisely using <em>frequency tables</em>. For each color of ball (red or green), each color of cube (blue or yellow) and each label, the tables have an entry telling us how many boxes of that kind are in the crate. For example:
    </p> 
  </div>

   <div class="section">
    <iframe src="2.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

  <div class="text-section">
    <p>
    The frequency tables specify a particular crate-and-boxes problem because they specify which boxes are in the crate. The number of boxes labeled <em>A</em> and contain a red ball and blue cube are displayed in the first table's upper left quadrant. Boxes which are labeled <em>B</em> and contain a green ball and a yellow cube are displayed in the second table's lower right quadrant. And so on. 
    <br><br>
    From this, we can infer other information. For example: we can sum up all the numbers in the tables to arrive at the number of boxes in total; we can sum up just the numbers in the first table for boxes labeled <em>A</em>; just the numbers in the second table in the first row for boxes labeled <em>B</em> and containing a red ball; sum up just the numbers in the the third table's second column for boxes labeled <em>C</em> and containing a yellow cube. 
    <br><br>
    This particular crate-and-boxes problem will be our running example. We'll get to know it well.
    <br><br>
    </p>
    <h2>Why think about crates and boxes?</h2>
    <p>
    Crate-and-boxes problems are odd. Why bother thinking about them? Because they're clear and simple models of problems we care about, problems which come up all over the place in practice.
    <br><br>
    For example, take the problem of predicting whether people will commit a crime, which is the problem <em>compas</em> tries to solve. Here's the abstract structure: 
    <br><br>
    <div class="quote">
    There is some population of people. You learn some things about each person: their criminal record, age and qualifications, for example. Each person either will or won't commit a crime and each person is either white or black. Based on what you learn about a person, you predict whether they'll commit a crime. You want to ensure your predictions aren't biased in favor of white people over black people or vice versa.
    </div>
    <br><br>
    Let's abstract further:
    <br><br>
    </p>
    <div class="quote">
    There is some collection of objects. You learn some features about each object. Each object also falls into one of two classes (positive or negative, say) and one of two groups (0 or 1, say). Based on what you learn about an object, you predict its class. You want to ensure your predictions aren't biased in favor of one group over the other.
    </div>
    <br><br>
    <p>
    Problems having that abstract structure come up all over the place. Take industrial quality control: You predict whether a product is defective (class) on the basis of its dimensions (features), where some products are produced in the night-shift and some in the day-shift (group). Or a job application: You predict whether an applicant will do well at the company (class) on the basis of their performance on a test (features), where some applicants are men and some are women (group). Or medical diagnosis: You predict whether a patient has a disease (class) on the basis of their symptoms (features), where some patients are over and some under fifty (group). And so on, and on, and on. Problems like these are everywhere.
    <br><br>
    All of them can be modeled by crate-and-boxes problems. To spell out the model:
    </p>
    <br><br>
      <table>
        <tr>
          <th>crate-and-boxes problem</th>
          <th>in the abstract</th> 
          <th>example: compas</th>
        </tr>
        <tr>
          <td>boxes<br>color of ball: red or green<br>color of cube: blue or yellow label</td>
          <td>objects<br>class: positive or negative<br>group: 0 or 1 features</td> 
          <td>people<br>will commit crime: yes or no<br>race: white or black<br>results of questionnaire</td>
        </tr>
      </table>
    <br><br>
    <p>
    Because crate-and-boxes problems model all these practical problems, results proved about crate-and-boxes problems apply to these practical problems too. In addition, thinking about crates and boxes is easier than thinking about the practical problems, as we'll see. That's why it's worth thinking about them.
    </p>
    <h2>How probability comes into the picture</h2>
    <p>
    How does probability come into the picture in all this? In practical problems, probability can come into the picture in several ways. I'll give two examples.
    <br><br>
    First, the feature, class or group of an object might be a probabilistic matter, like the outcome of a coin flip. Suppose I'm playing blackjack, say. You predict whether I'll win a hand (class) on the basis of the face-up cards (features), where sometimes I'm dealt a royal card and sometimes not (group). In this case, all three things --- feature, class, group --- are probabilistic. Of course, either I'll win or I won't, either the face-up cards will be these or those or so on, and either I'll be dealt a royal card or not. But more can be said: about the probabilities of winning, of the face-up cards, of being dealt a royal card. To represent the problem, you should represent the probabilities in your model.
    <br><br>
    Second, even if the problem itself is not probabilistic, you might be uncertain about it --- uncertain about the feature, class or group of the object. Uncertainty can be represented with probabilities. If you want to model not the problem itself but your uncertainty about the problem --- how the problem looks from your point of view --- then your model will involve probabilities.
    <br><br>
    Most practical problems will involve probabilities, one way or another. To model the problems properly, we need to represent the probabilities.
    <br><br>
    In a crate-and-boxes problem, Mac draws a box at random from the crate. Drawing the boxes is the one and only place that probability comes into a crate-and-boxes problem. Still, crate-and-boxes problems can model practical problems no matter how probabilities come into the practical problems. What matters is that the <em>probabilities </em> in a crate-and-boxes problem match the <em>probabilities</em> in the practical problem which it models. For example, to model the problem faced by <em>compas</em>, we need to come up with a crate-and-boxes problem in which the probability that Mac draws a box containing a red ball and a blue cube equals the probability that <em>compas</em> is applied to someone who will go on to commit a crime and is white, and so on. That can always be arranged: just pick the right frequency tables for the crate. It doesn't matter whether the <em>source</em> of the probabilities in the crate-and-boxes problem matches the <em>source</em> of the probabilities in the practical problem.
    <br><br>
    In short, crate-and-boxes problems can model all the problems we're interested in, whatever the sources of probability, and do so in a clear and simple way.
    </p>

    <h2>Working out probabilities</h2>
    <p>
    People (e.g. Chouldechova 2017, Kleinberg et al. 2017) have proved interesting things about crate-and-boxes problems. In order to understand the results, we need to get comfortable working out probabilities. Let's focus on our running example, the crate-and-boxes problem in Figure 1.
    <br><br>
    Mac draws a box uniformly at random from the crate. That means all of the 25 boxes have an equal chance, 1 in 25, or 4%, of being drawn. It's like drawing names from a hat. That's a helpful feature of crate-and-boxes problems. It makes life simple. It means that working out probabilities just comes down to counting.
    <br><br>
    Examples. The probability Mac draws a box labeled <em>A</em> that contains a red ball and blue cube is 2 in 25, because there are 2 boxes of that kind and 25 boxes in total. Similarly, the probability Mac draws a box labeled <em>B</em> that contains a green ball and a yellow cube is 3 in 25, the probability Mac draws a box labeled <em>A</em> is 7 in 25, and the probability Mac draws a box labeled <em>C</em> that contains a yellow cube is 2 in 25. 
    <br><br>
    We can get fancier. The probability Mac draws a box which is either labeled <em>A</em> or contains a red ball is 16 in 25. We can get fancier still. What's the probability Mac draws a box that contains a red ball, given that she draws a box labeled <em>A</em>? Well, there are 7 boxes labeled <em>A</em> and of these 7 boxes 4 contain a red ball. So it's 4 in 7. Similarly, the probability Mac draws a box that contains a yellow cube, given that she draws a box containing a green ball, is 5 in 12. And the probability Mac draws a box that contains a green ball and a yellow cube, given that she draws a box labeled <em>B</em> or containing a blue cube, is 3 in 20. 
    <br><br>
    When we have the tables describing a crate, calculating probabilities is easy. Just count.
    </p>

    <h2>Predictions and strategies</h2>
    <p>
    Remember Mac's task in a crate-and-boxes problem: she draws a box at random, reads its label and predicts the color of the ball inside. If the boxes were unlocked, she could just open them and check the color of the ball directly. But life is hard. She has to proceed indirectly, by predicting the color of the ball on the basis of the label.
    <br><br>
    I haven't specified what <em>kinds</em> of predictions Mac makes. She might make categorical predictions: red or green, like a jury's verdict in court. Or she might make more sophisticated predictions. For example, she might assign a score from 1-10, as <em>compas</em> does, where a higher score represents a higher chance that the ball is red. For now, let's suppose she makes categorical predictions.
    <br><br>
    It's helpful to distinguish <em>predictions</em> and <em>strategies</em>. After Mac draws a box from the crate, she makes a prediction -- red or green -- based on its label. Before she draws a box from the crate, she doesn't know what the label will be. But she can decide for each label what to predict if she draws a box with that label. She's deciding her strategy. A strategy, then, is a complete contingency plan: it lists Mac's prediction for each label. Because Mac makes categorical predictions, the strategies take the form: if <em>label</em>, then <em>color</em>. Let's call these strategies <em>decision rules</em>.
    <br><br>
    Don't focus on Mac --- what she knows about the crate, what she thinks of the decision rules, or anything else. Mac's role is just to draw the boxes. Focus on the decision rules themselves. We're interested in properties of the decision rules, because we want to find out which decision rules, if any, are fair.
     <br><br>   
    Which properties? Back to our running example. There are eight possible decision rules: if <em>A</em> predict red and if <em>B</em> predict red and if <em>C</em> predict red; if <em>A</em> predict red and if <em>B</em> predict red and if <em>C</em> predict green; if <em>A</em> predict red and if <em>B</em> predict green and if <em>C</em> predict red; and so on. For each decision rule we can ask, <em>What's the probability of predicting incorrectly when using it?</em> That property --- let's call it a decision rule's <em>error probability</em> --- is one of the properties we'll be interested in.
    <br><br>
    For each decision rule we can also ask, <em>What's the probability of predicting incorrectly when using it, given that Mac draws a box containing a blue cube?</em>. That property --- let's call it a decision rule's <em>blue error probability</em> --- is like the error probability, but among boxes containing a blue cube instead of among all boxes. Similarly, a decision rule's <em>yellow error probability</em> is the probability of predicting incorrectly when using it, given that Mac draws a box containing a yellow cube.
    <br><br>
    This will be the general pattern in what follows: we'll think about properties of decision rules --- among all boxes, among blue cube boxes, and among yellow cube boxes.
    <br><br>
    To work out blue and yellow error probabilities:
    <br><br>
    Take the first decision rule in our running example: if <em>A</em> predict red, if <em>B</em> predict red, if <em>C</em> predict red. What's the probability of predicting incorrectly when using it? Well, when using that decision rule the prediction is wrong just if Mac draws a box containing a green ball. The probability Mac draws a box containing a green ball is 12 in 25, or 48%. So the decision rule's error probability is 48%. 
    <br><br>
    Take the second decision rule: if $A$ predict red, if <em>B</em> predict red, if <em>C</em> predict green. What's the probability of predicting incorrectly when using it? Well, when using that decision rule the prediction is wrong just if Mac draws a box labeled $A$ and containing a green ball or Mac draws a box labeled <em>B</em> and containing a green ball or Mac draws a box labeled <em>C</em> and containing a red ball. The probability of that is 14 in 25, or 56%. So the decision rule's error probability is 56%. 
    <br><br>
    Similarly, we can work out the error probabilities of the other six decision rules.
    <br><br>
    As it turns out, the decision rule which has the lowest error probability is: if <em>A</em> predict red, if <em>B</em> predict green, if <em>C</em> predict red. (Exercise: What is its error probability?) You might have guessed this just by looking at the tables. Among the boxes labeled <em>A</em>, more are red than green; among the boxes labeled <em>B</em>, more are green than red; among the boxes labeled <em>C</em>, more are red than green. So it makes sense that predicting red for boxes labeled <em>A</em> or <em>C</em> and green for boxes labeled <em>B</em> has the lowest error probability.
    <br><br>
    We've worked out some overall error probabilities. Now let's work out some blue and yellow error probabilities. Take the first decision rule. What's the probability of predicting incorrectly when using it, given that Mac draws a box containing a blue cube? Well, as we already noted, when using that decision rule the prediction is wrong just if Mac draws a box containing a green ball. There are 15 boxes containing a blue cube and among these 15 boxes 7 contain a green ball. So the decision rule's blue error probability is 7 in 15, or about 47%. Similarly, its yellow error probability is 5 in 10, or 50%. Note that the blue and yellow error probabilities are different: the yellow is higher than the blue.
    <br><br>
    Take the second decision rule. As we already noted, when using that decision rule the prediction is wrong just if Mac draws a box labeled $A$ and containing a green ball or Mac draws a box labeled $B$ and containing a green ball or Mac draws a box labeled $C$ and containing a red ball. There are 15 boxes containing a blue cube and among these 15 boxes 2 are labeled $A$ and contain a green ball, and 4 are labeled $B$ and contain a green ball, and 3 are labeled $C$ and contain a red ball. So the decision rule's blue error probability is 9 in 15, or 60%. Similarly, its yellow error probability is 5 in 10, or 50%. Again, the blue and yellow error probabilities are different, but this time the blue is higher than the yellow.
    <br><br>
    Exercise: Work out the blue and yellow error probabilities for the rule: if <em>A</em> predict green, if $B$ predict red, if <em>C</em> predict green.
    </p>
  </div>

  <div class="text-section">
    <h2>Form of results</h2>
    <div class="quote">
      In any crate-and-boxes problem, no decision rule has <em>such-and-such</em> properties (unless the crate happens to be like <em>so-and-so</em>).
    </div>
    <p>
    The result is interesting because it looks as though, in the practical problems modeled by the crate-and-boxes problems, a decision rule which lacks the properties is unfair. If no decision rule has the properties, then no decision rule is fair.
    <br><br>
    The coming sections will fill in the place-holders: the <em>such-and-such</em> and the <em>so-and-so</em>.
    </p>
    <h1>Properties of strategies</h1>
    <h2>Notation</h2>
   
    <p>To save time and ink, let's introduce some notation. For example, instead of writing</p>
   
    <div class="quote">
    The probability Mac draws a box containing a green ball and yellow cube, given that she draws a box labeled <em>B</em> or containing a blue cube, is 3 in 20.
    </div>
    
    <p>let's write</p>
    
    <div class="quote">
    P(Ball = green, Cube = yellow <em>|</em> Label = <em>B</em> or Cube = blue) = 3/20.
    </div>

    <p>Or even more briefly,</p>

    <div class="quote">
    P(B=g, C=y $|$ L=$B$ or C=b) = 3/11.
    </div>

    <p>Similarly, given a particular decision rule, instead of writing</p>

    <div class="quote">
    The probability of incorrectly predicting red when using that decision rule is 48%.
    </div>

    <p>we can write</p>

    <div class="quote">
     P(B=g, X=r) = 48%.
    </div>

    <p>And so on. I use <em>X</em> instead of <em>P</em> to stand for the prediction, since we already use <em>P</em> to stand for probability.
    </p>

    <h2>Properties of decision rules</h2>

    <p>
      In Section 2.5, we looked at a property of decision rules, the error probability. Remember: given a crate, a decision rule's error probability is the probability of predicting incorrectly when using it. Let's define some more properties --- eight more of them in fact. To keep track of them all, we'll put them in a table:
    </p>

  </div>



   <div class="section">
    <iframe src="3.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

</body>
<script>
scroll=document.querySelector('.scroll');
setup = document.querySelector('#start');
nav = document.querySelector('.nav');
scroll.onclick=function() {
    setup.scrollIntoView({ 
    behavior: 'smooth',
    block: "start",
    // inline:"start"
  });
}

w =window.innerWidth;
h = window.innerHeight;
function resize() {
  iframes = document.querySelectorAll('iframe');
  for(i=0;i<iframes.length;i++){
    iframes[i].width=w;
    iframes[i].height=h;
  }
}

resize();

console.log(w,h)

var ismobile;
if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
 ismobile = true;
} else {
  ismobile = false;
}

if(ismobile==false){
  window.onresize=function() {
    location.reload();
  }
}

// float up text
text = document.querySelectorAll('.text-section')
window.onscroll=function() {
  for(i=0;i<text.length;i++){
    console.log(text[i])
    bounding = text[i].getBoundingClientRect();
    if (
      bounding.top > 0 || bounding.bottom > window.innerHeight*0.5
    ) {
      text[i].style.opacity="1";
      text[i].style.transform="translateY(0em)";
    } else {
      text[i].style.opacity="0";
      text[i].style.transform="translateY(5em)";
    }
  }
}

</script>


</html>