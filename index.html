<!DOCTYPE html>
<html>
<head>
  <title></title>
 <meta name="viewport" content="width=device-width,height=device-height">
  <script src="https://d3js.org/d3.v4.min.js" charset="utf-8"></script>
    <link href="https://fonts.googleapis.com/css?family=Playfair+Display:400,700,900" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet">
<style>
::-webkit-scrollbar {
    display:none;
    width: 0px;
    background: transparent; /* make scrollbar transparent */
}
::-webkit-scrollbar-track
{
    background-color: transparent;
}
::-webkit-scrollbar-thumb
{
    background-color: transparent;
}
body, html {
  font-family: 'Playfair Display', serif;
  width:100%;
  overflow-x:hidden;
  margin:0;
  left:0;
  top:0;
/*  scroll-snap-type: y mandatory;*/
}
h1 {
  font-weight:400;
}
em {
  font-style:normal;
  font-weight:900;
}
a{
  color:#572EFF;
  text-decoration:none;
}
.section {
  width:100%;
  height:100vh;
/*  scroll-snap-align: start;*/
  animation: fadeIn 1s ease-in-out;
}
.text-section {
  width:90%;
  height:auto;
  padding:5%;
  line-height:2em;
  animation:fadeIn 1s ease-in-out;
}
.quote {
  font-size:1.5em;
  color:#9C9198;
  border-left:solid #572EFF 0.25em;
  padding-left:5%;

}
#title {
  height:100vh;
}

.scroll {
  font-stretch:expanded;
  position:absolute;
  width:100%;
  bottom:5%;
  text-align:center;
  font-size:6em;
  color:#572EFF;
  cursor:pointer;
  transition:0.5s all ease-in-out;
}
.scroll:hover {
  transform:scale(1.2);
  transform-origin:50% 30%;
}
table {
  width:90%;
}
th {
  text-align:left;
}
td {
}
@keyframes fadeIn {
  0% {opacity:0;}
  100% {opacity:1;}
}

/*iphone5*/
@media screen and (max-width:320px){
  .scroll {
    bottom:-5%;
  }
}
/*mobile*/
@media screen and (max-width:767px){
/*  body, html {
  scroll-snap-type: none;
  }
  .section {
    scroll-snap-align: none;
  }*/
  .scroll {
    bottom:0%;
  }
}
</style>
</head>
<body>

  <div class="section" id="title">
     <iframe src="title.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
     <div class="scroll">&#xfe40;</div>
  </div>

  <div class="text-section" id="start">
      <h1>Big Picture</h1>
      <h2>Automating Predictions</h2>
      <p>
      The Allegheny Family Screening Tool is a computer program that predicts whether a child will later have to be placed into foster care. It's been used in Allegheny County, Pennsylvania, since August 2016. When a child is referred to the county as at risk of abuse or neglect, the program analyzes administrative records and then outputs a score from 1 to 20, where a higher score represents a higher risk that the child will later have to be placed into foster care. Child welfare workers use the score to help them decide whether to investigate a case further.
      <br><br>
      Travel search engines like Kayak or Google Flights predict whether a flight will go up or down in price. Farecast, which launched in 2004 and was acquired by Microsoft a few years later, was the first to offer such a service. When you look up a flight, these search engines analyze price records and then predict whether the flight's price will go up or down over some time interval, perhaps along with a measure of confidence in the prediction. People use the predictions to help them decide when to buy a ticket.
      <br><br>
      <em>compas</em> is a computer program that predicts whether a person will go on to commit a crime. It's widely used in the justice system in the United States. When someone is arrested in a jurisdiction that uses the program, prison staff input information about them and then  <em>compas</em> outputs a score from 1 to 10, where a higher score represents a higher risk that they'll go on to commit a crime. In some states, judges use the scores to help them make decisions about bond amounts, sentencing and parole.
      <br><br>
      The Allegheny Family Screening Tool, flight price forecasts and <em>compas</em> are examples of <em>automated prediction tools.</em> Such tools are becoming more and more common in all sorts of fields: from healthcare to the criminal justice system, from education to sports analytics, from social services to shopping. They typically go hand in hand with big data, machine learning and AI.
      <br><br>
      On the one hand, you might hope that using automated prediction tools will lead to better predictions: more accurate, because based on huge datasets; more accountable, because arrived at by explicit calculation; more fair, because free of human biases. Just as AlphaGo plays better Go than any person, so too will automated prediction tools make better predictions than any person --- or so you might hope.
      <br><br>
      On the other hand, you might worry that using these tools will lead to worse predictions: less accurate, because using fixed and ill-chosen criteria; less accountable, because the details are proprietary or opaque; less fair, because reflecting the biases of the dataset and the designers. Just as people are better at writing jokes or doing cryptic crosswords than any program, so too with making predictions --- or so you might worry.

      <h2>Aim of the post</h2>
      This post is about <em>fairness</em>. In particular, it’s about some interesting recent results (e.g. Chouldechova 2017, Kleinberg et al. 2017), results which came out of attempts to check whether particular automated prediction tools were fair, but which seem to have a more general consequence: that in a wide variety of situations it’s impossible to make fair predictions. As Kleinberg et al. put it in their abstract: “These results suggest some of the ways in which key notions of fairness are incompatible with each other”.
      <br><br>
      The aim of the post is to make these results accessible to a general audience, without sacrificing rigor or generality. By the end, you'll understand the results and be well-positioned to follow up on them yourself. And by smoothing away the technicalities, I hope to help bring out some of the conceptual issues.
      <br><br>
      The literature on fairness in automated prediction tools is extensive and rapidly growing. For example: some researchers develop formal definitions of fairness; some try to produce fairer prediction tools; and some argue that we should forget about formalizing fairness and instead focus on the broader social and political implications of using automated prediction tools in a given context. The results which I explain in this post belong to the first of these research programs: formal definitions of fairness.

      <h2>An example: COMPAS</h2>
      The best way into the topic is through a concrete example. Take one of the prediction tools above, <em>compas</em>, which predicts whether a person will go on to commit a crime.

      The information input to <em>compas</em> is gathered from a questionnaire, some of which is filled out by prison staff and some by the subject themselves. Here's <a href="documentcloud.org/documents/2702103-Sample-Risk-Assessment-COMPASCORE.html">a questionnaire used in Wisconsin.</a> Questions include: "How many times has this person been sentenced to jail for 30 days or more?", "If you lived with both parents and they later separated, how old were you at the time?", "How often do you have barely enough money to get by?", "To what extent do you agree or disagree that a hungry person has a right to steal?". After you input the information from the questionnaire, <em>compas</em> outputs a score. We don't know how exactly it does this because it's produced and sold by a private company (formerly Northpointe, now <a href="https://www.equivant.com/northpointe-suite">Equivant </a> and the details of how it works are proprietary.
      <br><br>
      How should we evaluate <em>compas</em>? Here's a straightforward approach: gather the risk scores <em>compas</em> assigned in lots of actual cases, find out in each case whether the subject did go on to commit a crime, and check the data for any worrying patterns. 
      <br><br>
      Of course, it's <a href="https://www.themarshallproject.org/2014/12/04/the-misleading-math-of-recidivism">not in fact straightforward</a> to check whether someone did go on to commit a crime. So most studies use a proxy, such as whether someone was re-arrested within, say, two years, or got another conviction. Using proxies like these raises all sorts of serious questions: How well do the proxies track the truth? How reliable is the data about the proxies themselves? How do we combine studies which used different proxies?  And there is a further, more general, worry too: making a prediction about someone might causally affect whether that very prediction comes true, because, for example, it might affect whether that person is incarcerated or not, or their parole conditions. Despite these caveats, the idea is clear: a direct way to evaluate <em>compas</em> is to analyze how its predictions held up in a large number of cases.
      <br><br>
      Investigative journalists at ProPublica did exactly that. They gathered data about over 7000 cases in Broward County, Florida, and checked it for any worrying patterns. They found some. In May 2016, they published <a href="https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing">an article</a> in which they claimed that <em>compas</em> is ``biased against blacks''. Here are some of the patterns they found:
      <br><br>
      <table>
        <tr>
          <th>Criteria</th>
          <th>White</th> 
          <th>African American</th>
        </tr>
        <tr>
          <td>score was 5-10 but didn't go on to commit a crime</td>
          <td>23.5%</td> 
          <td>44.9%</td>
        </tr>
        <tr>
          <td>score was 1-4 but did go on to commit a crime</td>
          <td>47.7%</td> 
          <td>28.0%</td>
        </tr>
      </table>
      <br><br>
      What is the table telling us? Focus first on the white subjects in ProPublica's data. Around a quarter of them were judged to have a high risk of committing a crime (a score of 5-10) but in fact didn't commit a crime. Around half of them were judged to have a low risk of committing a crime (a score of 1-4) but in fact did. Now focus on the black subjects. The situation is roughly the opposite. Around half were judged to have a high risk of committing a crime but in fact didn't. Around a quarter were judged to have a low risk of committing a crime but in fact did. 
      <br><br>
      We might say, quite naturally, that when <em>compas</em> gives a high score to someone who doesn't commit a crime or a low score to someone who does, that it's made a mistake. When <em>compas</em> makes a mistake with white people, it tends to be by giving low scores. When it makes a mistake with black people, it tends to be by giving high scores. 
      <br><br>
      The statistics look damning. They seem to show, as ProPublica claimed, that <em>compas</em> is biased against black people. (Here is Northpointe’s response. And here is ProPublica’s response to that.)
      <br><br>
      However, it turns out that things are not so clear. No one disputes ProPublica's statistics. But people do dispute whether the statistics demonstrate that <em>compas</em> is biased. Why? Because some recent results show that no matter how scores are assigned --- whether by human or machine, whether using algorithm, experience or intuition, whether based on a little evidence or a lot --- similar statistics will hold. Statistics like these are unavoidable. If statistics like these show that a prediction tool is unfair, then fair prediction tools are impossible.
      <br><br>
</p>
  </div>

  <div class="text-section">
    <h1>Crates and Boxes</h1>
    <h2>Crate-and-boxes problems</h2>
    <p>
    To think through the results, we’ll use a clean and clear abstraction: <em>crate-and-boxes problems</em>. The abstraction, free from distracting complexities, will help us think, in general, about how predictions can generate statistics like ProPublica’s. A crate-and-boxes problem works like this:
    <br><br>
    <div class="quote">Mac is facing a crate full of locked boxes. Each box is labeled with a letter. Inside each box is a ball, which is either red or green, and a cube, which is either blue or yellow. Mac draws a box at random from the crate, reads its label, and predicts the color of the ball inside.</div>
    <br><br>
    </p>
  </div>

  <div class="section" id="setup">
    <iframe src="1.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

  <div class="text-section">
    <p>
    To specify a particular crate-and-boxes problem, we need to specify which boxes are in the crate. For example, we need to specify how many boxes are labeled <em>A</em> and contain a red ball and a blue cube, how many boxes are labeled <em>B</em> and contain a green ball and a yellow cube, and so on. We can specify all these things concisely using <em>frequency tables</em>. For each color of ball (red or green), each color of cube (blue or yellow) and each label, the tables have an entry telling us how many boxes of that kind are in the crate. For example:
    </p> 
  </div>

   <div class="section">
    <iframe src="2.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

  <div class="text-section">
    <p>
    The frequency tables specify a particular crate-and-boxes problem because they specify which boxes are in the crate. The number of boxes labeled <em>A</em> and contain a red ball and blue cube are displayed in the first table's upper left quadrant. Boxes which are labeled <em>B</em> and contain a green ball and a yellow cube are displayed in the second table's lower right quadrant. And so on. 
    <br><br>
    From this, we can infer other information. For example: we can sum up all the numbers in the tables to arrive at the number of boxes in total; we can sum up just the numbers in the first table for boxes labeled <em>A</em>; just the numbers in the second table in the first row for boxes labeled <em>B</em> and containing a red ball; sum up just the numbers in the the third table's second column for boxes labeled <em>C</em> and containing a yellow cube. 
    <br><br>
    This particular crate-and-boxes problem will be our running example. We'll get to know it well.
    <br><br>
    </p>
    <h2>Why think about crates and boxes?</h2>
    <p>
    Crate-and-boxes problems are odd. Why bother thinking about them? Because they're clear and simple models of problems we care about, problems which come up all over the place in practice.
    <br><br>
    For example, take the problem of predicting whether people will commit a crime, which is the problem <em>compas</em> tries to solve. Here's the abstract structure: 
    <br><br>
    <div class="quote">
    There is some population of people. You learn some things about each person: their criminal record, age and qualifications, for example. Each person either will or won't commit a crime and each person is either white or black. Based on what you learn about a person, you predict whether they'll commit a crime. You want to ensure your predictions aren't biased in favor of white people over black people or vice versa.
    </div>
    <br><br>
    Let's abstract further:
    </p>
  </div>


   <div class="section">
    <iframe src="3.html" width=1280 height=720 frameborder=0 scrolling=no></iframe>
  </div>

</body>
<script>
scroll=document.querySelector('.scroll');
setup = document.querySelector('#start');
nav = document.querySelector('.nav');
scroll.onclick=function() {
    setup.scrollIntoView({ 
    behavior: 'smooth',
    block: "start",
    // inline:"start"
  });
}

w =window.innerWidth;
h = window.innerHeight;
function resize() {
  iframes = document.querySelectorAll('iframe');
  for(i=0;i<iframes.length;i++){
    iframes[i].width=w;
    iframes[i].height=h;
  }
}

resize();

console.log(w,h)

var ismobile;
if( /Android|webOS|iPhone|iPad|iPod|BlackBerry|IEMobile|Opera Mini/i.test(navigator.userAgent) ) {
 ismobile = true;
} else {
  ismobile = false;
}

if(ismobile==false){
  window.onresize=function() {
    location.reload();
  }
}
</script>


</html>